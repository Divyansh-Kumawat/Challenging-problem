# Challenging-problem

This project is an Autonomous QA Testing Framework built for an E-Shop Checkout application. It automates the entire QA workflow, including understanding requirements, generating grounded test cases using AI, and executing them with Selenium. The framework uses a RAG pipeline that reads documents like product_specs.md, ui_ux_guide.txt, and checkout.html to ensure all test cases strictly follow the actual specifications. A Streamlit app is included so users can upload documents, build a knowledge base, generate test cases, and automatically produce Selenium scripts.
One of the toughest challenges was ensuring the AI did not hallucinate requirements, features, or selectors while generating test cases. Models often add fields or rules that do not exist, which can break Selenium automation and reduce test reliability.
To solve this, I built a strict grounding system that validated every requirement and selector against the real documentation. I parsed the HTML to extract all valid IDs and names and blocked any test case referencing non-existent elements. I also chunked all documents using embeddings to ensure only relevant sections were retrieved during generation. Each test case includes mandatory "Grounded_In" citations, which map every step back to the exact source document.
This approach ensured 100% spec-accurate, non-hallucinated test cases and stable Selenium execution. The final system produces consistent, high-quality automation scripts that align perfectly with the application's real behavior.
